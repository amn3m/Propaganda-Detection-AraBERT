{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e8e1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "# You want to see \"True\" for CUDA Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb18723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed\\OneDrive\\Desktop\\NLP\\NLP_Project_Propaganda\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training on Device: CUDA\n",
      "   GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "   VRAM: 6.44 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ Training on Device: {device.upper()}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # Check VRAM (Memory)\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"   VRAM: {vram:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: You are running on CPU. This will be very slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc8606b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Loaded successfully.\n",
      "   - Training Samples: 5073\n",
      "   - Testing Samples:  1269\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "data_path = r\"C:\\Users\\Ahmed\\OneDrive\\Desktop\\NLP\\NLP_Project_Propaganda\\data\\processed\\arabic_propaganda_dataset.csv\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"‚ùå File not found: {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# 2. Rename columns for Hugging Face\n",
    "df = df[['Text', 'Final_Label']].rename(columns={'Text': 'text', 'Final_Label': 'label'})\n",
    "\n",
    "# 3. Map Labels to Numbers\n",
    "label_map = {'Non-Propaganda': 0, 'Propaganda': 1}\n",
    "df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# 4. Split (80% Train, 20% Test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# 5. Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(f\"‚úÖ Data Loaded successfully.\")\n",
    "print(f\"   - Training Samples: {len(train_df)}\")\n",
    "print(f\"   - Testing Samples:  {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b549f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Tokenizing data... (Please wait)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5073/5073 [00:00<00:00, 15865.78 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1269/1269 [00:00<00:00, 15838.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load AraBERT Tokenizer\n",
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Truncation=True and max_length=128 are critical for 6GB VRAM\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "print(\"‚è≥ Tokenizing data... (Please wait)\")\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "print(\"‚úÖ Tokenization Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04869f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading AraBERT Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded to GPU.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Metric Function (F1 Macro is required)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"f1_macro\": f1, \"accuracy\": acc}\n",
    "\n",
    "# 2. Load Model to GPU\n",
    "print(\"‚è≥ Loading AraBERT Model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "print(\"‚úÖ Model loaded to GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e536bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer Initialized.\n"
     ]
    }
   ],
   "source": [
    "# Optimized for RTX 3050 (6GB VRAM)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,              # How many times to look at the data\n",
    "    per_device_train_batch_size=4,   # Small batch size to avoid crash\n",
    "    per_device_eval_batch_size=8,    # Evaluation uses less memory, so 8 is fine\n",
    "    gradient_accumulation_steps=4,   # Accumulate 4 steps = Effective Batch Size 16\n",
    "    eval_strategy=\"epoch\",           # <--- CHANGED FROM evaluation_strategy\n",
    "    save_strategy=\"epoch\",           # Save model every epoch\n",
    "    learning_rate=3e-5,\n",
    "    fp16=True,                       # Mixed Precision (Saves memory & speeds up training)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,              # Only keep the last 2 models to save disk space\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"‚úÖ Trainer Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b608f3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='954' max='954' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [954/954 05:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.647100</td>\n",
       "      <td>0.638636</td>\n",
       "      <td>0.395426</td>\n",
       "      <td>0.654058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.635127</td>\n",
       "      <td>0.464504</td>\n",
       "      <td>0.661151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.636727</td>\n",
       "      <td>0.508330</td>\n",
       "      <td>0.653270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=954, training_loss=0.643982821290598, metrics={'train_runtime': 304.7118, 'train_samples_per_second': 49.946, 'train_steps_per_second': 3.131, 'total_flos': 1001071787880960.0, 'train_loss': 0.643982821290598, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üöÄ Starting Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762dc11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Test Set Evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [159/159 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6367270350456238, 'eval_f1_macro': 0.508330104610616, 'eval_accuracy': 0.653270291568164, 'eval_runtime': 4.1562, 'eval_samples_per_second': 305.328, 'eval_steps_per_second': 38.256, 'epoch': 3.0}\n",
      "------------------------------------------------\n",
      "‚úÖ Model Saved successfully to: C:\\Users\\Ahmed\\OneDrive\\Desktop\\NLP\\NLP_Project_Propaganda\\models\\arabert_propaganda_model\n",
      "üèÜ Final F1 Score: 0.5083\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Final Evaluation\n",
    "print(\"\\nüìä Final Test Set Evaluation:\")\n",
    "stats = trainer.evaluate()\n",
    "print(stats)\n",
    "\n",
    "# 2. Save the Best Model\n",
    "save_path = r\"C:\\Users\\Ahmed\\OneDrive\\Desktop\\NLP\\NLP_Project_Propaganda\\models\\arabert_propaganda_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(f\"‚úÖ Model Saved successfully to: {save_path}\")\n",
    "print(f\"üèÜ Final F1 Score: {stats['eval_f1_macro']:.4f}\")\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09725e",
   "metadata": {},
   "source": [
    "# Phase 2 Report: Comparative Study & Implementation\n",
    "\n",
    "**Project Title:** Propaganda Detection in Arabic Narratives (Idea 6)\n",
    "**Course:** AIS411 - NLP\n",
    "**Phase Status:** ‚úÖ Completed\n",
    "**Date:** January 3, 2026\n",
    "\n",
    "## 1. Objective\n",
    "The goal of Phase 2 was to implement and compare two distinct classification models to detect propaganda in Arabic text. This satisfies the course requirement: *\"Comparative study and implementation, each member implements a different model.\"*\n",
    "\n",
    "## 2. Models Implemented\n",
    "\n",
    "### **Model A: Deep Learning (Transformer)**\n",
    "* **Architecture:** `AraBERT-Base-v02` (Pre-trained BERT model for Arabic).\n",
    "* **Why Chosen:** State-of-the-art performance on Arabic NLP tasks due to its ability to capture contextual meaning.\n",
    "* **Training Setup:**\n",
    "    * **Framework:** Hugging Face Transformers & PyTorch.\n",
    "    * **Optimization:** Mixed Precision (FP16) + Gradient Accumulation (steps=4) to accommodate the RTX 3050 6GB GPU.\n",
    "    * **Hyperparameters:** Epochs=3, Batch Size=4 (Effective 16), Learning Rate=3e-5.\n",
    "\n",
    "### **Model B: Baseline (Classical ML)**\n",
    "* **Architecture:** Logistic Regression.\n",
    "* **Feature Extraction:** TF-IDF (Term Frequency-Inverse Document Frequency) limited to top 5,000 features.\n",
    "* **Why Chosen:** Serves as a fast, interpretable benchmark to measure the \"lift\" provided by Deep Learning and to establish a performance floor.\n",
    "* **Configuration:** `class_weight='balanced'` was explicitly used to address the dataset's 2:1 class imbalance.\n",
    "\n",
    "## 3. Comparative Results\n",
    "\n",
    "| Metric | Model A (AraBERT) | Model B (Baseline) | Interpretation |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Accuracy** | **~65.3%** | 54.5% | AraBERT achieves higher overall accuracy by predicting the majority class frequently. |\n",
    "| **F1 Macro** | 0.508 | **0.526** | **Baseline Wins.** The statistical model better handles the minority class due to explicit class weighting. |\n",
    "| **Training Time** | ~15 minutes (GPU) | < 10 seconds (CPU) | Baseline is significantly more computationally efficient. |\n",
    "\n",
    "## 4. Analysis & Conclusion\n",
    "* **Imbalance Challenge:** The dataset contains significantly more \"Propaganda\" samples (4,150) than \"Non-Propaganda\" samples (2,192).\n",
    "* **Model Behavior:**\n",
    "    * **AraBERT:** Converged towards a \"majority class classifier,\" achieving high accuracy (65%) by essentially guessing \"Propaganda\" for most inputs. This \"laziness\" resulted in a lower F1 Macro score (0.508).\n",
    "    * **Baseline:** The use of `class_weight='balanced'` penalized mistakes on the minority class. While this lowered total accuracy (more false positives), it resulted in a more robust F1 Macro score (0.526), making it a \"fairer\" classifier.\n",
    "* **Verdict:** Deep Learning (AraBERT) has higher potential capacity but requires specific optimization techniques (like Class Weights or Loss Function modification) to handle imbalanced data effectively. This optimization is the primary objective for Phase 3.\n",
    "\n",
    "## 5. Deliverables\n",
    "* **Saved Models:**\n",
    "    * `models/arabert_propaganda_model/` (PyTorch Weights & Tokenizer)\n",
    "    * `models/baseline_model.pkl` (Scikit-Learn Pipeline)\n",
    "* **Notebooks:**\n",
    "    * `02_Model_Training_AraBERT.ipynb`: Deep Learning implementation.\n",
    "    * `03_Model_Comparison_Baseline.ipynb`: Classical ML implementation.\n",
    "\n",
    "---\n",
    "### Next Step: Phase 3 (Optimization)\n",
    "The next phase will focus on surpassing the baseline by forcing AraBERT to learn from the minority class.\n",
    "* **Action:** Implement **Class Weights** in the Trainer to penalize the model for ignoring \"Non-Propaganda\" samples.\n",
    "* **Target:** F1 Macro > 0.60."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595388e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
